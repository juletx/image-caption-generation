{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"},"colab":{"name":"P71_OIER_Caption_Generation.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true}},"cells":[{"cell_type":"markdown","metadata":{"id":"EgKik34_yLCf"},"source":["# Project 71: Automatic Caption Generation of Images\n","\n","## Authors: Oihane Cantero and Julen Etxaniz\n","\n","## Supervisors: Oier Lopez de Lacalle and Eneko Agirre\n","\n","## Subject: Machine Learning and Neural Networks\n","\n","## Date: 15-12-2020\n","\n","## Objectives: \n","### 1. Implement from scratch a caption generation model that uses a CNN to condition a LSTM based language model.\n","### 2. Extend the basic caption generation system that incorporates an attention mechanism to the model.\n","\n","## Contents:\n","### Prepare Colab\n","### Import Libraries\n","### Prepare Photo Data\n","### Prepare Text Data\n","### Load Data\n","### Encode Text Data\n","### Define Model\n","### Fit Model\n","### Evaluate Model\n","### Generate Captions"]},{"cell_type":"markdown","metadata":{"id":"u_zf45TWiqaz"},"source":["# Prepare Colab"]},{"cell_type":"code","metadata":{"id":"jTknrvCi9Ugl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607347271365,"user_tz":-60,"elapsed":35547,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"949fdc71-80b9-4c5d-a92f-447d5a155996"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IqLvN2Jo-XKg","executionInfo":{"status":"ok","timestamp":1607347275026,"user_tz":-60,"elapsed":39200,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"a3e288e0-dc30-44c2-dae3-af0396e3ef20"},"source":["%cd /content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/1. Lauhilekoa/MLNN/Projects/Topic 3 Deep Neural Networks/neural-caption-generation/notebook"],"execution_count":4,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Ingeniaritza Informatikoa/4. Maila/1. Lauhilekoa/MLNN/Projects/Topic 3 Deep Neural Networks/neural-caption-generation/notebook\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4VjEGAqcS0Rs","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607248820500,"user_tz":-60,"elapsed":582,"user":{"displayName":"Oihane C","photoUrl":"","userId":"13420400780517162820"}},"outputId":"ef20ff9f-a0dc-4c4e-89e1-3a36621e9796"},"source":["%cd /content/drive/MyDrive/4/MLNN/P3/neural-caption-generation/notebook"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/.shortcut-targets-by-id/1yKqinuPu4oBVyhJ-8IMgknNyE2cmqrcs/neural-caption-generation/notebook\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"8hqD4zjMcCVa"},"source":["# Import Libraries\n","\n"]},{"cell_type":"markdown","metadata":{"id":"UWTsX3mZ7D2X"},"source":["Prepare Photo Data"]},{"cell_type":"code","metadata":{"id":"WNxC1FVQUusn","executionInfo":{"status":"ok","timestamp":1607347239003,"user_tz":-60,"elapsed":3212,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from os import listdir\n","from os.path import isfile\n","from pickle import dump\n","from keras.applications.vgg16 import VGG16\n","from keras.preprocessing.image import load_img\n","from keras.preprocessing.image import img_to_array\n","from keras.applications.vgg16 import preprocess_input\n","from keras.models import Model\n","# better image model\n","# from keras.applications.inception_v3 import InceptionV3 \n","# from keras.applications.inception_v3 import preprocess_input"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fszyNBc17HMa"},"source":["Prepare Text Data"]},{"cell_type":"code","metadata":{"id":"n0U1CRDpwWFy","executionInfo":{"status":"ok","timestamp":1607347239005,"user_tz":-60,"elapsed":3205,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["import string"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HH29iX6v7KGj"},"source":["Load Data"]},{"cell_type":"code","metadata":{"id":"Y8aTjn6Qz7fC","executionInfo":{"status":"ok","timestamp":1607347277424,"user_tz":-60,"elapsed":2393,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from numpy import array\n","from pickle import load"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EmAHAn7Esm5X"},"source":["Encode Text Data"]},{"cell_type":"code","metadata":{"id":"k0AUZ-nPsq04","executionInfo":{"status":"ok","timestamp":1607347277425,"user_tz":-60,"elapsed":2384,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from keras.preprocessing.text import Tokenizer"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8wU34qn37d_M"},"source":["Define Model"]},{"cell_type":"code","metadata":{"id":"-DS2AYyt7b90","executionInfo":{"status":"ok","timestamp":1607347277426,"user_tz":-60,"elapsed":1540,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from keras.utils import plot_model\n","from keras.models import Model\n","from keras.models import load_model\n","from keras.layers import Input\n","from keras.layers import Dense\n","from keras.layers import LSTM\n","from keras.layers import Embedding\n","from keras.layers import Dropout\n","from keras.layers.merge import add"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wvrWqvabtUzr"},"source":["Fit Model"]},{"cell_type":"code","metadata":{"id":"HrsPGdYbtWnU","executionInfo":{"status":"ok","timestamp":1607347277887,"user_tz":-60,"elapsed":1206,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from keras.preprocessing.sequence import pad_sequences\n","from keras.utils import to_categorical"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TayWgM7Z7f6P"},"source":["Evaluate Model"]},{"cell_type":"code","metadata":{"id":"6XPHfWRY6SHX","executionInfo":{"status":"ok","timestamp":1607347281223,"user_tz":-60,"elapsed":3811,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from numpy import argmax, argsort\n","from nltk.translate.bleu_score import corpus_bleu"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BUaqK3j-7k5I"},"source":["Generate Captions"]},{"cell_type":"code","metadata":{"id":"bXRjGEHSC_9s","executionInfo":{"status":"ok","timestamp":1607347475180,"user_tz":-60,"elapsed":1134,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["from IPython.display import Image, display"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oYgi-5WYAZWK"},"source":["# Prepare Photo Data"]},{"cell_type":"code","metadata":{"id":"SqcweYB7tpd1","executionInfo":{"status":"ok","timestamp":1607347447635,"user_tz":-60,"elapsed":859,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# get VGG16 model\n","def get_model():\n","    # load the model\n","    model = VGG16()\n","    # re-structure the model\n","    model = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","    # summarize\n","    print(model.summary())\n","    return model"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"MRzBWbsoCEzq","executionInfo":{"status":"ok","timestamp":1607347448974,"user_tz":-60,"elapsed":683,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# preprocess the image for the VGG model\n","def preprocess_image(filename):\n","    image = load_img(filename, target_size=(224, 224))\n","    # convert the image pixels to a numpy array\n","    image = img_to_array(image)\n","    # reshape data for the model\n","    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n","    # prepare the image for the VGG model\n","    image = preprocess_input(image)\n","    return image"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"MYNng7dKAfhT"},"source":["# extract features from each photo in the directory\n","def extract_features(directory):\n","\t# get VGG16 model\n","\tmodel = get_model()\n","\t# extract features from each photo\n","\tfeatures = dict()\n","\tfor name in listdir(directory):\n","\t\t# load an image from file\n","\t\tfilename = directory + '/' + name\n","\t\t# preprocess the image for the VGG model\n","\t\timage = preprocess_image(filename)\n","\t\t# get features\n","\t\tfeature = model.predict(image, verbose=0)\n","\t\t# get image id\n","\t\timage_id = name.split('.')[0]\n","\t\t# store feature\n","\t\tfeatures[image_id] = feature\n","\t\tprint('>%s' % name)\n","\treturn features"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h398spr-Ag_0"},"source":["# extract features from all images\n","directory = 'Flickr8k_Dataset'\n","features = extract_features(directory)\n","print('Extracted Features: %d' % len(features))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0a-X8QZNwLIH"},"source":["# save to file\n","filename = 'files/features.pkl'\n","if not isfile(filename):\n","\tdump(features, open(filename, 'wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iMa4SfKpuVwO"},"source":["# Prepare Text Data"]},{"cell_type":"code","metadata":{"id":"fbsiUSEIm1y7","executionInfo":{"status":"ok","timestamp":1607347287823,"user_tz":-60,"elapsed":1052,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# load doc into memory\n","def load_doc(filename):\n","\t# open the file as read only\n","\tfile = open(filename, 'r')\n","\t# read all text\n","\ttext = file.read()\n","\t# close the file\n","\tfile.close()\n","\treturn text"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3EMc3znwmw0"},"source":["# extract descriptions for images\n","def load_descriptions(doc):\n","\tmapping = dict()\n","\t# process lines\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\tif len(line) < 2:\n","\t\t\tcontinue\n","\t\t# take the first token as the image id, the rest as the description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# remove filename from image id\n","\t\timage_id = image_id.split('.')[0]\n","\t\t# convert description tokens back to string\n","\t\timage_desc = ' '.join(image_desc)\n","\t\t# create the list if needed\n","\t\tif image_id not in mapping:\n","\t\t\tmapping[image_id] = list()\n","\t\t# store description\n","\t\tmapping[image_id].append(image_desc)\n","\treturn mapping"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YGSP5RlRwoqr"},"source":["def clean_descriptions(descriptions):\n","\t# prepare translation table for removing punctuation\n","\ttable = str.maketrans('', '', string.punctuation)\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor i in range(len(desc_list)):\n","\t\t\tdesc = desc_list[i]\n","\t\t\t# tokenize\n","\t\t\tdesc = desc.split()\n","\t\t\t# convert to lower case\n","\t\t\tdesc = [word.lower() for word in desc]\n","\t\t\t# remove punctuation from each token\n","\t\t\tdesc = [w.translate(table) for w in desc]\n","\t\t\t# remove hanging 's' and 'a'\n","\t\t\tdesc = [word for word in desc if len(word)>1]\n","\t\t\t# remove tokens with numbers in them\n","\t\t\tdesc = [word for word in desc if word.isalpha()]\n","\t\t\t# store as string\n","\t\t\tdesc_list[i] =  ' '.join(desc)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GqMNBVP_wqVP"},"source":["# convert the loaded descriptions into a vocabulary of words\n","def to_vocabulary(descriptions):\n","\t# build a list of all description strings\n","\tall_desc = set()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n","\treturn all_desc"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UEYqYFXGwyfT"},"source":["# save descriptions to file, one per line\n","def save_descriptions(descriptions, filename):\n","\tlines = list()\n","\tfor key, desc_list in descriptions.items():\n","\t\tfor desc in desc_list:\n","\t\t\tlines.append(key + ' ' + desc)\n","\tdata = '\\n'.join(lines)\n","\tfile = open(filename, 'w')\n","\tfile.write(data)\n","\tfile.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2gqCKWhBw2lH"},"source":["filename = 'Flickr8k_text/Flickr8k.token.txt'\n","# load descriptions\n","doc = load_doc(filename)\n","# parse descriptions\n","descriptions = load_descriptions(doc)\n","print('Loaded: %d ' % len(descriptions))\n","# clean descriptions\n","clean_descriptions(descriptions)\n","# summarize vocabulary\n","vocabulary = to_vocabulary(descriptions)\n","print('Vocabulary Size: %d' % len(vocabulary))\n","# save to file\n","save_descriptions(descriptions, 'files/descriptions.txt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MY-0uSBMpg4v"},"source":["# Load Data"]},{"cell_type":"code","metadata":{"id":"aSGIwKgE1uh9","executionInfo":{"status":"ok","timestamp":1607347294150,"user_tz":-60,"elapsed":1082,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# load a pre-defined list of photo identifiers\n","def load_set(filename):\n","\tdoc = load_doc(filename)\n","\tdataset = list()\n","\t# process line by line\n","\tfor line in doc.split('\\n'):\n","\t\t# skip empty lines\n","\t\tif len(line) < 1:\n","\t\t\tcontinue\n","\t\t# get the image identifier\n","\t\tidentifier = line.split('.')[0]\n","\t\tdataset.append(identifier)\n","\treturn set(dataset)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"TI4GVM2O1wnm","executionInfo":{"status":"ok","timestamp":1607347295742,"user_tz":-60,"elapsed":1161,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# load clean descriptions into memory\n","def load_clean_descriptions(filename, dataset):\n","\t# load document\n","\tdoc = load_doc(filename)\n","\tdescriptions = dict()\n","\tfor line in doc.split('\\n'):\n","\t\t# split line by white space\n","\t\ttokens = line.split()\n","\t\t# split id from description\n","\t\timage_id, image_desc = tokens[0], tokens[1:]\n","\t\t# skip images not in the set\n","\t\tif image_id in dataset:\n","\t\t\t# create list\n","\t\t\tif image_id not in descriptions:\n","\t\t\t\tdescriptions[image_id] = list()\n","\t\t\t# wrap description in tokens\n","\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n","\t\t\t# store\n","\t\t\tdescriptions[image_id].append(desc)\n","\treturn descriptions"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"egBvy5YP1zJs","executionInfo":{"status":"ok","timestamp":1607347297275,"user_tz":-60,"elapsed":1509,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# load photo features\n","def load_photo_features(filename, dataset):\n","\t# load all features\n","\tall_features = load(open(filename, 'rb'))\n","\t# filter features\n","\tfeatures = {k: all_features[k] for k in dataset}\n","\treturn features"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"id":"HXBve5EZ2Uvk","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607347303888,"user_tz":-60,"elapsed":5748,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"97065515-5c75-4302-82b6-8344f6567d06"},"source":["# load training dataset (6K)\n","filename = 'Flickr8k_text/Flickr_8k.trainImages.txt'\n","train = load_set(filename)\n","print('Dataset: %d' % len(train))\n","# descriptions\n","train_descriptions = load_clean_descriptions('files/descriptions.txt', train)\n","print('Descriptions: train=%d' % len(train_descriptions))\n","# photo features\n","train_features = load_photo_features('files/features.pkl', train)\n","print('Photos: train=%d' % len(train_features))"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Dataset: 6000\n","Descriptions: train=6000\n","Photos: train=6000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GusN09bh2Z4d","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607347307739,"user_tz":-60,"elapsed":3842,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"ad037342-8d35-42dd-faca-a7c573441db7"},"source":["# load val dataset\n","filename = 'Flickr8k_text/Flickr_8k.devImages.txt'\n","val = load_set(filename)\n","print('Dataset: %d' % len(val))\n","# descriptions\n","val_descriptions = load_clean_descriptions('files/descriptions.txt', val)\n","print('Descriptions: val=%d' % len(val_descriptions))\n","# photo features\n","val_features = load_photo_features('files/features.pkl', val)\n","print('Photos: val=%d' % len(val_features))"],"execution_count":15,"outputs":[{"output_type":"stream","text":["Dataset: 1000\n","Descriptions: val=1000\n","Photos: val=1000\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"m7kO9alxrhJz"},"source":["# Encode Text Data"]},{"cell_type":"code","metadata":{"id":"w3ShJOv6102H","executionInfo":{"status":"ok","timestamp":1607347307740,"user_tz":-60,"elapsed":1015,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# covert a dictionary of clean descriptions to a list of descriptions\n","def to_lines(descriptions):\n","\tall_desc = list()\n","\tfor key in descriptions.keys():\n","\t\t[all_desc.append(d) for d in descriptions[key]]\n","\treturn all_desc"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"LO_nL4JK2A8g","executionInfo":{"status":"ok","timestamp":1607347308364,"user_tz":-60,"elapsed":744,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# fit a tokenizer given caption descriptions\n","def create_tokenizer(descriptions):\n","\tlines = to_lines(descriptions)\n","\ttokenizer = Tokenizer()\n","\ttokenizer.fit_on_texts(lines)\n","\treturn tokenizer"],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"tYDiiaSf2Dfb","executionInfo":{"status":"ok","timestamp":1607347311053,"user_tz":-60,"elapsed":1839,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# calculate the length of the description with the most words\n","def max_length(descriptions):\n","\tlines = to_lines(descriptions)\n","\treturn max(len(d.split()) for d in lines)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"TFOZjTf4_crV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607347314169,"user_tz":-60,"elapsed":2410,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"bf46a6f8-7112-4eee-f36d-0628bc66d54d"},"source":["# prepare tokenizer\n","tokenizer = create_tokenizer(train_descriptions)\n","# save the tokenizer\n","dump(tokenizer, open('files/tokenizer.pkl', 'wb'))\n","# define vocabulary size\n","vocab_size = len(tokenizer.word_index) + 1\n","print('Vocabulary Size: %d' % vocab_size)\n","# determine the maximum sequence length\n","max_length = max_length(train_descriptions)\n","print('Description Length: %d' % max_length)"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Vocabulary Size: 7579\n","Description Length: 34\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"GIIBFjgf2zl4"},"source":["# Define Model"]},{"cell_type":"code","metadata":{"id":"mkMCDMdh22pQ","executionInfo":{"status":"ok","timestamp":1607347355723,"user_tz":-60,"elapsed":579,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# define the captioning model\n","def define_model(vocab_size, max_length):\n","\t# feature extractor model\n","\tinputs1 = Input(shape=(4096,))\n","\tfe1 = Dropout(0.5)(inputs1)\n","\tfe2 = Dense(256, activation='relu')(fe1)\n","\t# sequence model\n","\tinputs2 = Input(shape=(max_length,))\n","\tse1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n","\tse2 = Dropout(0.5)(se1)\n","\tse3 = LSTM(256)(se2)\n","\t# decoder model\n","\tdecoder1 = add([fe2, se3])\n","\tdecoder2 = Dense(256, activation='relu')(decoder1)\n","\toutputs = Dense(vocab_size, activation='softmax')(decoder2)\n","\t# tie it together [image, seq] [word]\n","\tmodel = Model(inputs=[inputs1, inputs2], outputs=outputs)\n","\tmodel.compile(loss='categorical_crossentropy', optimizer='adam')\n","\t# summarize model\n","\tprint(model.summary())\n","\tplot_model(model, to_file='files/model.png', show_shapes=True)\n","\treturn model"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"tiorbnol3i6-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607347362247,"user_tz":-60,"elapsed":4811,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7883b68a-2c48-455b-8bb5-50f89985ed02"},"source":["# define the model\n","model = define_model(vocab_size, max_length)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["Model: \"functional_1\"\n","__________________________________________________________________________________________________\n","Layer (type)                    Output Shape         Param #     Connected to                     \n","==================================================================================================\n","input_2 (InputLayer)            [(None, 34)]         0                                            \n","__________________________________________________________________________________________________\n","input_1 (InputLayer)            [(None, 4096)]       0                                            \n","__________________________________________________________________________________________________\n","embedding (Embedding)           (None, 34, 256)      1940224     input_2[0][0]                    \n","__________________________________________________________________________________________________\n","dropout (Dropout)               (None, 4096)         0           input_1[0][0]                    \n","__________________________________________________________________________________________________\n","dropout_1 (Dropout)             (None, 34, 256)      0           embedding[0][0]                  \n","__________________________________________________________________________________________________\n","dense (Dense)                   (None, 256)          1048832     dropout[0][0]                    \n","__________________________________________________________________________________________________\n","lstm (LSTM)                     (None, 256)          525312      dropout_1[0][0]                  \n","__________________________________________________________________________________________________\n","add (Add)                       (None, 256)          0           dense[0][0]                      \n","                                                                 lstm[0][0]                       \n","__________________________________________________________________________________________________\n","dense_1 (Dense)                 (None, 256)          65792       add[0][0]                        \n","__________________________________________________________________________________________________\n","dense_2 (Dense)                 (None, 7579)         1947803     dense_1[0][0]                    \n","==================================================================================================\n","Total params: 5,527,963\n","Trainable params: 5,527,963\n","Non-trainable params: 0\n","__________________________________________________________________________________________________\n","None\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"SctfW2tY3UWz"},"source":["# Fit Model"]},{"cell_type":"code","metadata":{"id":"G_cVKyc4jBtz","executionInfo":{"status":"ok","timestamp":1607347365547,"user_tz":-60,"elapsed":857,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# create sequences of images, input sequences and output words for an image\n","def create_sequences(tokenizer, max_length, desc_list, photo, vocab_size):\n","\tX1, X2, y = list(), list(), list()\n","\t# walk through each description for the image\n","\tfor desc in desc_list:\n","\t\t# encode the sequence\n","\t\tseq = tokenizer.texts_to_sequences([desc])[0]\n","\t\t# split one sequence into multiple X,y pairs\n","\t\tfor i in range(1, len(seq)):\n","\t\t\t# split into input and output pair\n","\t\t\tin_seq, out_seq = seq[:i], seq[i]\n","\t\t\t# pad input sequence\n","\t\t\tin_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n","\t\t\t# encode output sequence\n","\t\t\tout_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n","\t\t\t# store\n","\t\t\tX1.append(photo)\n","\t\t\tX2.append(in_seq)\n","\t\t\ty.append(out_seq)\n","\treturn array(X1), array(X2), array(y)"],"execution_count":22,"outputs":[]},{"cell_type":"code","metadata":{"id":"P49MRhoQ38dS","executionInfo":{"status":"ok","timestamp":1607347366906,"user_tz":-60,"elapsed":703,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# data generator, intended to be used in a call to model.fit()\n","def data_generator(descriptions, photos, tokenizer, max_length, vocab_size):\n","\t# loop for ever over images\n","\twhile 1:\n","\t\tfor key, desc_list in descriptions.items():\n","\t\t\t# retrieve the photo feature\n","\t\t\tphoto = photos[key][0]\n","\t\t\tin_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo, vocab_size)\n","\t\t\tyield ([in_img, in_seq], out_word)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"P0T7On703tnV"},"source":["# train the model, run epochs manually and save after each epoch\n","epochs = 20\n","train_steps = len(train_descriptions)\n","val_steps = len(val_descriptions)\n","for i in range(epochs):\n","\t# create the train data generator\n","\tgenerator = data_generator(train_descriptions, train_features, tokenizer, max_length, vocab_size)\n","\t# create the validation data generator\n","\tvalidation_generator = data_generator(val_descriptions, val_features, tokenizer, max_length, vocab_size)\n","\t# fit for one epoch\n","\tmodel.fit(generator, validation_data=validation_generator, validation_steps=val_steps, epochs=1, steps_per_epoch=train_steps, verbose=1)\n","\t# save model\n","\tmodel.save('weights/model_' + str(i) + '.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ydnEMTVS4Q8m"},"source":["# Evaluate Model"]},{"cell_type":"code","metadata":{"id":"weKkjRJf4h4W","executionInfo":{"status":"ok","timestamp":1607347371169,"user_tz":-60,"elapsed":1014,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# map an integer to a word\n","def word_for_id(integer, tokenizer):\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == integer:\n","\t\t\treturn word\n","\treturn None"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"0aY50Rgb4i48","executionInfo":{"status":"ok","timestamp":1607347371604,"user_tz":-60,"elapsed":570,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","\t# seed the generation process\n","\tin_text = 'startseq'\n","\t# iterate over the whole length of the sequence\n","\tfor i in range(max_length):\n","\t\t# integer encode input sequence\n","\t\tsequence = tokenizer.texts_to_sequences([in_text])[0]\n","\t\t# pad input\n","\t\tsequence = pad_sequences([sequence], maxlen=max_length)\n","\t\t# predict next word\n","\t\tyhat = model.predict([photo,sequence], verbose=0)\n","\t\t# convert probability to integer\n","\t\tyhat = argmax(yhat)\n","\t\t# map integer to word\n","\t\tword = word_for_id(yhat, tokenizer)\n","\t\t# stop if we cannot map the word\n","\t\tif word is None:\n","\t\t\tbreak\n","\t\t# append as input for generating the next word\n","\t\tin_text += ' ' + word\n","\t\t# stop if we predict the end of the sequence\n","\t\tif word == 'endseq':\n","\t\t\tbreak\n","\treturn in_text"],"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"vZ7BVLByRv4l","executionInfo":{"status":"ok","timestamp":1607347375788,"user_tz":-60,"elapsed":788,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# generate a description for an image using beam search\n","def generate_desc_beam_search(model, tokenizer, photo, max_length, beam_index=3):\n","    # seed the generation process\n","    in_text = [['startseq', 0.0]]\n","    # iterate over the whole length of the sequence\n","    for i in range(max_length):\n","        temp = []\n","        for s in in_text:\n","            # integer encode input sequence\n","            sequence = tokenizer.texts_to_sequences([s[0]])[0]\n","            # pad input\n","            sequence = pad_sequences([sequence], maxlen=max_length)\n","            # predict next words\n","            preds = model.predict([photo,sequence], verbose=0)\n","            word_preds = argsort(preds[0])[-beam_index:]\n","            # get top predictions\n","            for w in word_preds:\n","                next_cap, prob = s[0][:], s[1]\n","                # map integer to word\n","                word = word_for_id(w, tokenizer)\n","                next_cap += ' ' + word\n","                prob += preds[0][w]\n","                temp.append([next_cap, prob])\n","\n","        in_text = temp\n","        # sorting according to the probabilities\n","        in_text = sorted(in_text, reverse=False, key=lambda l: l[1])\n","        # getting the top words\n","        in_text = in_text[-beam_index:]\n","\n","    # get last caption text\n","    in_text = in_text[-1][0]\n","    caption_list = []\n","    # remove leftover endseq \n","    for w in in_text.split():\n","        caption_list.append(w)\n","        if w == 'endseq':\n","            break\n","    # convert list to string\n","    caption = ' '.join(caption_list)\n","    return caption"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZhfD7pcp5jPL","executionInfo":{"status":"ok","timestamp":1607347382738,"user_tz":-60,"elapsed":959,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def calculate_scores(actual, predicted):\n","    # calculate BLEU score\n","\tprint('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n","\tprint('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n","\tprint('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n","\tprint('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"bNUMtYGdseKX","executionInfo":{"status":"ok","timestamp":1607347383204,"user_tz":-60,"elapsed":1235,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# evaluate the skill of the model\n","def evaluate_model(model, descriptions, photos, tokenizer, max_length):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc_list in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc(model, tokenizer, photos[key], max_length)\n","\t\t# store actual and predicted\n","\t\treferences = [d.split() for d in desc_list]\n","\t\tactual.append(references)\n","\t\tpredicted.append(yhat.split())\n","\tprint('Sampling:')\n","\tcalculate_scores(actual, predicted)"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"kHYAywFWwi2I","executionInfo":{"status":"ok","timestamp":1607347384016,"user_tz":-60,"elapsed":886,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# evaluate the skill of the model\n","def evaluate_model_beam_search(model, descriptions, photos, tokenizer, max_length, beam_index=3):\n","\tactual, predicted = list(), list()\n","\t# step over the whole set\n","\tfor key, desc_list in descriptions.items():\n","\t\t# generate description\n","\t\tyhat = generate_desc_beam_search(model, tokenizer, photos[key], max_length, beam_index)\n","\t\t# store actual and predicted\n","\t\treferences = [d.split() for d in desc_list]\n","\t\tactual.append(references)\n","\t\tpredicted.append(yhat.split())\n","\tprint('Beam Search k=%d:' % beam_index)\n","\tcalculate_scores(actual, predicted)"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"id":"P82D94XF_w86","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1607347387773,"user_tz":-60,"elapsed":1651,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}},"outputId":"7848706b-3e9c-4462-9ecd-50701299854d"},"source":["# load test set\n","filename = 'Flickr8k_text/Flickr_8k.testImages.txt'\n","test = load_set(filename)\n","print('Dataset: %d' % len(test))\n","# descriptions\n","test_descriptions = load_clean_descriptions('files/descriptions.txt', test)\n","print('Descriptions: test=%d' % len(test_descriptions))\n","# photo features\n","test_features = load_photo_features('files/features.pkl', test)\n","print('Photos: test=%d' % len(test_features))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Dataset: 1000\n","Descriptions: test=1000\n","Photos: test=1000\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"DNXL68Ja48H7"},"source":["# load the model\n","filename = 'weights/model_5.h5'\n","model = load_model(filename)\n","# evaluate model\n","evaluate_model(model, test_descriptions, test_features, tokenizer, max_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UAKgl4MR1dQ5"},"source":["evaluate_model_beam_search(model, test_descriptions, test_features, tokenizer, max_length, beam_index=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zZyLVed1hqE"},"source":["evaluate_model_beam_search(model, test_descriptions, test_features, tokenizer, max_length, beam_index=5)\n","evaluate_model_beam_search(model, test_descriptions, test_features, tokenizer, max_length, beam_index=7)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Az5FZbh_5IMR"},"source":["# Generate Captions"]},{"cell_type":"code","metadata":{"id":"ZlAbB4BuhM2S","executionInfo":{"status":"ok","timestamp":1607347394917,"user_tz":-60,"elapsed":875,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# extract features only from a picture\n","def extract_features(filename):\n","\t# load the model\n","\tmodel = VGG16()\n","\t# re-structure the model\n","\tmodel = Model(inputs=model.inputs, outputs=model.layers[-2].output)\n","\t# preprocess the image for the VGG model\n","\timage = preprocess_image(filename)\n","\t# get features\n","\tfeature = model.predict(image, verbose=0)\n","\treturn feature"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gY1JWr3-nS6","executionInfo":{"status":"ok","timestamp":1607347589768,"user_tz":-60,"elapsed":5402,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["# load the model\n","filename = 'weights/model_5.h5'\n","model = load_model(filename)\n","# load the tokenizer\n","tokenizer = load(open('files/tokenizer.pkl', 'rb'))"],"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"id":"RaCGUNh99MaB","executionInfo":{"status":"ok","timestamp":1607347593490,"user_tz":-60,"elapsed":1103,"user":{"displayName":"Julen Etxaniz Aragoneses","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GilpYDAmorj7KFdjK26TlsyH_HhFWhLqmESeKoCGsI=s64","userId":"06956422670240182492"}}},"source":["def generate_captions(directory):\n","    for name in listdir(directory):\n","        # load an image from file\n","        filename = directory + '/' + name\n","        photo = extract_features(filename)\n","        #diplay image\n","        display(Image(filename))\n","        # generate descriptions\n","        print('Sampling:')\n","        print(generate_desc(model, tokenizer, photo, max_length))\n","        print('Beam Search k=3:')\n","        print(generate_desc_beam_search(model, tokenizer, photo, max_length, beam_index=3))\n","        print('Beam Search k=5:')\n","        print(generate_desc_beam_search(model, tokenizer, photo, max_length, beam_index=5))\n","        print('Beam Search k=7:')\n","        print(generate_desc_beam_search(model, tokenizer, photo, max_length, beam_index=7))"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZYYtIX3V9tjK"},"source":["generate_captions('images')"],"execution_count":null,"outputs":[]}]}